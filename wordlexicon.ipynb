{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMLBudxz63BwS+QqcS6jzKP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install googletrans"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2NOa6HcoRrqy","executionInfo":{"status":"ok","timestamp":1763088722620,"user_tz":300,"elapsed":6216,"user":{"displayName":"Raveena Gupta","userId":"06103734461678506047"}},"outputId":"ed6fbda4-a7eb-4148-af9f-5b13da9f5e93"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting googletrans\n","  Downloading googletrans-4.0.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.27.2->googletrans) (0.28.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.11.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (0.16.0)\n","Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.27.2->googletrans) (4.3.0)\n","Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (6.1.0)\n","Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (4.1.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.3.1)\n","Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.15.0)\n","Downloading googletrans-4.0.2-py3-none-any.whl (18 kB)\n","Installing collected packages: googletrans\n","Successfully installed googletrans-4.0.2\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGT3IjuWNJYw","executionInfo":{"status":"ok","timestamp":1763089630327,"user_tz":300,"elapsed":4356,"user":{"displayName":"Raveena Gupta","userId":"06103734461678506047"}},"outputId":"c193f3bd-81f2-43d8-a613-5fed0bd5ccf5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Scraped 1000 words and saved to zulu_words_scraped.csv\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# URL of the webpage with the 1000 most common Zulu words\n","url = \"https://www.1000mostcommonwords.com/1000-most-common-zulu-words/\"\n","\n","# Send a GET request\n","response = requests.get(url)\n","response.raise_for_status()  # Make sure the request succeeded\n","\n","# Parse the HTML\n","soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","# Find the table containing the words\n","table = soup.find(\"table\")\n","\n","# Extract rows\n","rows = table.find_all(\"tr\")\n","\n","# Prepare list for storing pairs\n","words = []\n","\n","# Iterate over rows, skip header\n","for row in rows[1:]:\n","    cols = row.find_all(\"td\")\n","    if len(cols) >= 3:\n","        number = cols[0].text.strip()  # optional\n","        zulu_word = cols[1].text.strip()\n","        english_word = cols[2].text.strip()\n","        words.append((zulu_word, english_word))\n","\n","# Save to CSV\n","output_file = \"zulu_words_scraped.csv\"\n","with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"Zulu\", \"English\"])\n","    writer.writerows(words)\n","\n","print(f\"Scraped {len(words)} words and saved to {output_file}\")\n"]},{"cell_type":"code","source":["import csv\n","\n","# Load English sentiment lexicon (word -> valence)\n","english_sentiment = {\n","    \"happy\": 3,\n","    \"joy\": 4,\n","    \"sad\": -3,\n","    \"anger\": -4,\n","    \"love\": 3,\n","    \"hate\": -4,\n","    # Add more words or load from AFINN\n","}\n","\n","# Load scraped Zulu-English words\n","zulu_words = []\n","with open(\"zulu_words_scraped.csv\", newline=\"\", encoding=\"utf-8\") as f:\n","    reader = csv.DictReader(f)\n","    for row in reader:\n","        zulu_words.append((row[\"Zulu\"], row[\"English\"]))\n","\n","# Map sentiment\n","zulu_sentiment = []\n","for zulu, english in zulu_words:\n","    sentiment = english_sentiment.get(english.lower(), 0)  # default neutral = 0\n","    zulu_sentiment.append((zulu, english, sentiment))\n","\n","# Save CSV with sentiment\n","with open(\"zulu_words_with_sentiment.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"Zulu\", \"English\", \"Sentiment\"])\n","    writer.writerows(zulu_sentiment)\n","\n","\n"],"metadata":{"id":"YVW6gkEXVZyQ","executionInfo":{"status":"ok","timestamp":1763089687578,"user_tz":300,"elapsed":28,"user":{"displayName":"Raveena Gupta","userId":"06103734461678506047"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","import csv\n","from collections import defaultdict\n","\n","# -------------------------------\n","# Load dataset\n","# -------------------------------\n","zulu_dataset = load_dataset(\"michsethowusu/zulu-sentiments-corpus\")\n","zulu_sentences = zulu_dataset[\"train\"][\"Zulu\"][:300]\n","zulu_labels = zulu_dataset[\"train\"][\"sentiment\"][:300]\n","\n","# -------------------------------\n","# Collect word-level sentiment counts\n","# -------------------------------\n","word_scores = defaultdict(list)\n","\n","for sentence, label in zip(zulu_sentences, zulu_labels):\n","    sentiment_value = 3 if label.lower() == \"positive\" else -3\n","    words = sentence.split()\n","    for word in words:\n","        word_scores[word].append(sentiment_value)\n","\n","# -------------------------------\n","# Compute normalized sentiment\n","# -------------------------------\n","normalized_word_sentiment = []\n","for word, scores in word_scores.items():\n","    avg_sentiment = sum(scores) / len(scores)  # average across all occurrences\n","    normalized_word_sentiment.append([word, avg_sentiment])\n","\n","# -------------------------------\n","# Save to CSV\n","# -------------------------------\n","with open(\"zulu_word_sentiment_normalized.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"word\", \"sentiment\"])  # header\n","    writer.writerows(normalized_word_sentiment)\n","\n","print(f\"Saved {len(normalized_word_sentiment)} normalized word-level sentiment entries to zulu_word_sentiment_normalized.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PoFePmkmZraj","executionInfo":{"status":"ok","timestamp":1763094608941,"user_tz":300,"elapsed":854,"user":{"displayName":"Raveena Gupta","userId":"06103734461678506047"}},"outputId":"82477bf5-1624-46d5-8fee-303f3b169ead"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved 1850 normalized word-level sentiment entries to zulu_word_sentiment_normalized.csv\n"]}]},{"cell_type":"code","source":["!pip install evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L_38CkaHecLA","executionInfo":{"status":"ok","timestamp":1763092094434,"user_tz":300,"elapsed":5391,"user":{"displayName":"Raveena Gupta","userId":"06103734461678506047"}},"outputId":"08b8dfed-8c16-4b31-bc97-a7ee2785d9e4"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting evaluate\n","  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n","Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.6\n"]}]},{"cell_type":"code","source":["# =========================================\n","# Zulu Word Sentiment Regression Head with Custom NN\n","# =========================================\n","\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import AutoTokenizer, AutoModel\n","from tqdm import tqdm\n","\n","# -------------------------------\n","# 1️⃣ Load lexicon CSV\n","# -------------------------------\n","df = pd.read_csv(\"zulu_word_sentiment.csv\")\n","print(df.head())\n","\n","words = df[\"word\"].tolist()\n","labels = df[\"sentiment\"].astype(float).tolist()\n","\n","# -------------------------------\n","# 2️⃣ Load tokenizer and embeddings\n","# -------------------------------\n","model_name = \"Davlan/afro-xlmr-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","base_model = AutoModel.from_pretrained(model_name)\n","base_model.eval()  # freeze base\n","\n","# -------------------------------\n","# 3️⃣ Tokenize words and get embeddings\n","# -------------------------------\n","inputs = tokenizer(words, return_tensors=\"pt\", padding=True, truncation=True)\n","with torch.no_grad():\n","    outputs = base_model(**inputs)\n","    # Use the [CLS] token embedding for each word\n","    embeddings = outputs.last_hidden_state[:, 0, :]\n","\n","labels_tensor = torch.tensor(labels).unsqueeze(1)  # shape [N,1]\n","\n","# -------------------------------\n","# 4️⃣ Create simple regression head\n","# -------------------------------\n","class SentimentHead(nn.Module):\n","    def __init__(self, input_dim):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(input_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 1)\n","        )\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","regression_head = SentimentHead(embeddings.shape[1])\n","\n","# -------------------------------\n","# 5️⃣ Training setup\n","# -------------------------------\n","dataset = TensorDataset(embeddings, labels_tensor)\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n","\n","optimizer = torch.optim.Adam(regression_head.parameters(), lr=1e-3)\n","loss_fn = nn.MSELoss()\n","\n","# -------------------------------\n","# 6️⃣ Train\n","# -------------------------------\n","regression_head.train()\n","for epoch in range(20):\n","    total_loss = 0\n","    for batch_x, batch_y in dataloader:\n","        optimizer.zero_grad()\n","        preds = regression_head(batch_x)\n","        loss = loss_fn(preds, batch_y)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n","\n","# -------------------------------\n","# 7️⃣ Inference\n","# -------------------------------\n","def predict_sentiment(word):\n","    base_model.eval()\n","    regression_head.eval()\n","    with torch.no_grad():\n","        inputs = tokenizer(word, return_tensors=\"pt\", padding=True, truncation=True)\n","        emb = base_model(**inputs).last_hidden_state[:, 0, :]\n","        score = regression_head(emb).item()\n","    return score\n","\n","example_word = \"jabula\"\n","print(f\"Sentiment score for '{example_word}': {predict_sentiment(example_word):.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cBwMIKZrdVwW","executionInfo":{"status":"ok","timestamp":1763095444299,"user_tz":300,"elapsed":53379,"user":{"displayName":"Raveena Gupta","userId":"06103734461678506047"}},"outputId":"74bdb8b9-d417-4362-8e70-cdbe2478fb40"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["            word  sentiment\n","0      niyokwaba         -3\n","1     eniyokwaba         -3\n","2      niyokwaba         -3\n","3     aniyukwaba         -3\n","4  eningeyukwaba         -3\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 8.6964\n","Epoch 2, Loss: 8.5594\n","Epoch 3, Loss: 8.4661\n","Epoch 4, Loss: 8.4073\n","Epoch 5, Loss: 8.3888\n","Epoch 6, Loss: 8.1771\n","Epoch 7, Loss: 8.0946\n","Epoch 8, Loss: 7.9329\n","Epoch 9, Loss: 7.8125\n","Epoch 10, Loss: 7.6899\n","Epoch 11, Loss: 7.5546\n","Epoch 12, Loss: 7.4666\n","Epoch 13, Loss: 7.2791\n","Epoch 14, Loss: 7.0718\n","Epoch 15, Loss: 6.6732\n","Epoch 16, Loss: 6.4531\n","Epoch 17, Loss: 6.3689\n","Epoch 18, Loss: 6.3419\n","Epoch 19, Loss: 5.9967\n","Epoch 20, Loss: 5.7522\n","Sentiment score for 'jabula': 1.49\n"]}]},{"cell_type":"code","source":["import torch\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModel\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# -------------------------------\n","# Load Zulu dataset\n","# -------------------------------\n","zulu_dataset = load_dataset(\"michsethowusu/zulu-sentiments-corpus\")\n","zulu_sentences = zulu_dataset[\"train\"][\"Zulu\"][:153]\n","zulu_labels = zulu_dataset[\"train\"][\"sentiment\"][:153]  # 'Positive' / 'Negative'\n","\n","# -------------------------------\n","# Load Afro-XLMR base and your trained head\n","# -------------------------------\n","model_name = \"Davlan/afro-xlmr-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","base_model = AutoModel.from_pretrained(model_name).to(device)\n","base_model.eval()\n","\n","# Assume `regression_head` is your trained head\n","regression_head.eval()\n","regression_head = regression_head.to(device)\n","\n","# -------------------------------\n","# Function to get sentence embeddings\n","# -------------------------------\n","def get_sentence_embedding(sentences):\n","    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","    with torch.no_grad():\n","        outputs = base_model(**inputs)\n","        # Use [CLS] embedding as sentence representation\n","        emb = outputs.last_hidden_state[:, 0, :]\n","    return emb\n","\n","# -------------------------------\n","# Zero-shot prediction\n","# -------------------------------\n","sentence_embs = get_sentence_embedding(zulu_sentences)\n","with torch.no_grad():\n","    scores = regression_head(sentence_embs).squeeze()  # shape [num_sentences]\n","    # Map scores to positive/negative\n","    preds = torch.where(scores >= 0, 1, 0)\n","\n","# -------------------------------\n","# Compute accuracy\n","# -------------------------------\n","label_map = {\"Negative\": 0, \"Positive\": 1}\n","zulu_labels_int = torch.tensor([label_map[l] for l in zulu_labels]).to(device)\n","accuracy = (preds == zulu_labels_int).float().mean().item()\n","print(f\"Zero-shot accuracy on Zulu samples: {accuracy:.4f}\")\n","\n","# # -------------------------------\n","# # Optional: print predictions\n","# # -------------------------------\n","# for sent, score, pred in zip(zulu_sentences, scores, preds):\n","#     print(f\"'{sent}' -> Score: {score:.2f}, Pred: {'Positive' if pred==1 else 'Negative'}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HcZH8dWIiamS","executionInfo":{"status":"ok","timestamp":1763096013073,"user_tz":300,"elapsed":3450,"user":{"displayName":"Raveena Gupta","userId":"06103734461678506047"}},"outputId":"3a26c6e6-7b8f-4bcb-9a91-d4804ee8c47d"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Zero-shot accuracy on Zulu samples: 0.6078\n"]}]}]}