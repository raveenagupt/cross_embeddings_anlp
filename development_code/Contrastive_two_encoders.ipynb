{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyP008yGSZDasL7Vp/GbR0ng"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"f3E9PYKg6dL2","executionInfo":{"status":"ok","timestamp":1763068271725,"user_tz":300,"elapsed":43,"user":{"displayName":"Raveena Gupta","userId":"06103734461678506047"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from transformers import AutoTokenizer, AutoModel\n","import numpy as np\n","from torch.utils.data import DataLoader, Dataset\n","\n","# -------------------------------\n","# 1. Dataset wrapper\n","# -------------------------------\n","class ZuluEnglishDataset(Dataset):\n","    def __init__(self, zulu_sentences, english_sentences):\n","        self.zulu_sentences = zulu_sentences\n","        self.english_sentences = english_sentences\n","\n","    def __len__(self):\n","        return len(self.zulu_sentences)\n","\n","    def __getitem__(self, idx):\n","        return self.zulu_sentences[idx], self.english_sentences[idx]\n","\n","# -------------------------------\n","# 2. Contrastive encoders\n","# -------------------------------\n","class ContrastiveEncoder(nn.Module):\n","    def __init__(self, model_name, proj_dim=256):\n","        super().__init__()\n","        self.encoder = AutoModel.from_pretrained(model_name)\n","        self.projection = nn.Linear(self.encoder.config.hidden_size, proj_dim)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n","        cls_emb = outputs.last_hidden_state[:, 0, :]\n","        proj = self.projection(cls_emb)\n","        return nn.functional.normalize(proj, p=2, dim=1)\n","\n","# -------------------------------\n","# 3. Contrastive Loss (InfoNCE)\n","# -------------------------------\n","def contrastive_loss(emb1, emb2, temperature=0.05):\n","    \"\"\"\n","    emb1, emb2: (batch_size, dim)\n","    \"\"\"\n","    batch_size = emb1.size(0)\n","    sim_matrix = torch.matmul(emb1, emb2.T) / temperature\n","    labels = torch.arange(batch_size).to(emb1.device)\n","    loss = nn.CrossEntropyLoss()(sim_matrix, labels)\n","    return loss\n","\n","# -------------------------------\n","# 4. Training loop skeleton\n","# -------------------------------\n","def train_contrastive(zulu_sentences, english_sentences, epochs=5, batch_size=16, proj_dim=256):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Load tokenizers and models\n","    tokenizer_zu = AutoTokenizer.from_pretrained(\"MoseliMotsoehli/zuBERTa\")\n","    tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","    model_zu = ContrastiveEncoder(\"MoseliMotsoehli/zuBERTa\", proj_dim).to(device)\n","    model_en = ContrastiveEncoder(\"bert-base-uncased\", proj_dim).to(device)\n","\n","    dataset = ZuluEnglishDataset(zulu_sentences, english_sentences)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","    optimizer = optim.Adam(list(model_zu.parameters()) + list(model_en.parameters()), lr=2e-5)\n","\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for zu_batch, en_batch in dataloader:\n","            # Tokenize\n","            zu_inputs = tokenizer_zu(list(zu_batch), return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n","            en_inputs = tokenizer_en(list(en_batch), return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n","\n","            # Forward\n","            emb_zu = model_zu(zu_inputs[\"input_ids\"], zu_inputs[\"attention_mask\"])\n","            emb_en = model_en(en_inputs[\"input_ids\"], en_inputs[\"attention_mask\"])\n","\n","            # Loss\n","            loss = contrastive_loss(emb_zu, emb_en)\n","\n","            # Backward\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n","\n","    return model_zu, model_en, tokenizer_zu, tokenizer_en, device\n","\n","# ========================\n","# Example usage\n","# ========================\n","# zulu_sentences, english_sentences = load_your_dataset()  # list of strings\n","# model_zu, model_en, tokenizer_zu, tokenizer_en, device = train_contrastive(zulu_sentences, english_sentences)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"8vH7n8CpDxjK"},"execution_count":null,"outputs":[]}]}