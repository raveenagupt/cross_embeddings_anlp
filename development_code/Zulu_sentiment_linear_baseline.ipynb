{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4149,"status":"ok","timestamp":1762996265764,"user":{"displayName":"Raveena Gupta","userId":"06103734461678506047"},"user_tz":300},"id":"MPr-x9LTI8LX","outputId":"f99a3c4f-94db-4b7f-be45-96e2572254fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"]}],"source":["!pip install gensim\n","import numpy as np\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","from gensim.models import KeyedVectors  # for fastText English embeddings\n","import json\n","import numpy as np\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","import kagglehub\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66624,"status":"ok","timestamp":1762999426909,"user":{"displayName":"Raveena Gupta","userId":"06103734461678506047"},"user_tz":300},"id":"n1SKzAlCIA4A","outputId":"73acd62a-c7b3-4501-ba1a-b17eb016b0f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Loaded 4739 sentence pairs from CSV.\n","\n","First 5 Zulu sentences:\n","1: Ama-albhamu akhe e-solo eminyaka yawo-60 angamanye anezingoma ezimnandi kakhulu kwengake ngazizwa.\n","2: Ukusuka lapho kuzobe sekulandela ukuthi uma ufuna ukusindisa inyonyana, uzobe uphoqelela ukuthi zonke izindawo ezingamazinki zenze isiqinisekiso sokuthi iBhrithani ishiye ngomhla zingama-31 ku-Okthoba.\n","3: Ngemuva kweminyaka engamashumi amathathu u-St.John Paul II enxuse abaseMozambique ukuthi bayiqede impi yabo yombango, uFrancis kulindeleke ukuthi asisayine lesi sivumelwano esisha sangomhla lulu-1 ku-Agasti aphinde akhuthaze ukuthi sifezekiswe ngokugcwele lapho ehlangana neziphathimandla zikahulumeni ngoLwesine, okuwusuku lwakhe lokuqala olugcwele esifundeni. Ufike ngoLwesithathu kusihlwa kodwa ubengenayo imicimbi yomphakathi ebihleliwe ngemuva komcimbi wakhe omfushane wokwamukelwa esikhumulweni sezindiza lapho bekushaywa khona izigubhu zomdabu, kudanswa kukikizelwa kunjeya.\n","4: Wynford yw sylfaenydd Stafell Fyw yng Nghaerdydd - canolfan sy'n helpu pobl â dibyniaeth - ac mae hefyd yn ymgynghorydd cwnsela arbenigol.\n","5: Iqembu lika-Unai Emery lisanqobe imidlalo emibili kuphela kwemihlanu ye-Premier League, futhi lisalelwe ngamaphuzu amathathu ukuze lingene ku-Champions League.\n","\n","First 5 English sentences:\n","1: His solo albums in the 60s were some of the most brilliant and breathtaking beautiful sounds I've ever heard.\n","2: From which it follows that if you want to save the union, you should be straining every sink to make sure Britain leaves on 31 October.\n","3: Thirty years after St. John Paul II begged Mozambicans to end their civil war, Francis is expected to endorse the new Aug. 1 accord and urge its full implementation when he meets with government authorities on Thursday, his first full day in the region. He arrived Wednesday evening but had no public events scheduled after his brief airport welcome ceremony that featured traditional drums, dance and ululating singers.\n","4: Wynford yw sylfaenydd Stafell Fyw yng Nghaerdydd - canolfan sy'n helpu pobl â dibyniaeth - ac mae hefyd yn ymgynghorydd cwnsela arbenigol.\n","5: Unai Emery's side have won just two of their last five Premier League games, and remain three points off the Champions League places.\n","Loading model: MoseliMotsoehli/zuBERTa ...\n","Loading model: bert-base-uncased ...\n","\n","Computing sentence embeddings...\n","Computed embeddings for 4739 pairs.\n","Computing linear projection (least squares)...\n","\n","========================================\n","Zulu sentence: Ama-albhamu akhe e-solo eminyaka yawo-60 angamanye anezingoma ezimnandi kakhulu kwengake ngazizwa.\n","Projected into English embedding space.\n","Predicted Sentiment: negative\n","========================================\n","Mean cosine similarity between projected Zulu and English embeddings: 0.840421\n"]}],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# ==========================================================\n","# 1. Model Utilities\n","# ==========================================================\n","def load_model(model_name: str):\n","    \"\"\"Load a pre-trained transformer model and tokenizer.\"\"\"\n","    print(f\"Loading model: {model_name} ...\")\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModel.from_pretrained(model_name)\n","    model.eval()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    return tokenizer, model, device\n","\n","\n","def get_sentence_embedding(sentence: str, tokenizer, model, device):\n","    \"\"\"Compute mean-pooled sentence embedding.\"\"\"\n","    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n","    return embedding\n","\n","\n","# ==========================================================\n","# 2. Projection Utilities\n","# ==========================================================\n","def compute_linear_projection(X, Y):\n","    \"\"\"Compute least-squares linear projection matrix W.\"\"\"\n","    print(\"Computing linear projection (least squares)...\")\n","    return np.linalg.pinv(X) @ Y\n","\n","\n","def project_sentence(sentence, W, tokenizer, model, device):\n","    \"\"\"Project a Zulu sentence into English embedding space.\"\"\"\n","    emb = get_sentence_embedding(sentence, tokenizer, model, device)\n","    return emb @ W\n","\n","\n","# ==========================================================\n","# 3. Dataset Utilities\n","# ==========================================================\n","def load_csv_zulu_dataset(csv_path: str):\n","    \"\"\"Load Zulu-English sentence pairs from CSV.\"\"\"\n","    df = pd.read_csv(csv_path)\n","    if \"zu\" not in df.columns or \"en\" not in df.columns:\n","        raise ValueError(\"CSV must have columns 'zu' for Zulu and 'en' for English sentences\")\n","    print(f\"✅ Loaded {len(df)} sentence pairs from CSV.\")\n","\n","    # Print first 5 examples\n","    print(\"\\nFirst 5 Zulu sentences:\")\n","    for i, s in enumerate(df[\"zu\"].head(5), 1):\n","        print(f\"{i}: {s}\")\n","\n","    print(\"\\nFirst 5 English sentences:\")\n","    for i, s in enumerate(df[\"en\"].head(5), 1):\n","        print(f\"{i}: {s}\")\n","\n","    return df[\"zu\"].tolist(), df[\"en\"].tolist()\n","\n","\n","# ==========================================================\n","# 4. Dummy Sentiment Classifier\n","# ==========================================================\n","class DummyEnglishSentimentClassifier:\n","    \"\"\"A trivial sentiment classifier based on sum of vector elements.\"\"\"\n","    def predict(self, X):\n","        return [\"positive\" if X.sum() > 0 else \"negative\"]\n","\n","\n","# ==========================================================\n","# 5. Main Pipeline\n","# ==========================================================\n","def run_pipeline(csv_path: str = \"/content/en-zu.training.csv\"):\n","    # Step 1: Load dataset\n","    zulu_sentences, english_sentences = load_csv_zulu_dataset(csv_path)\n","\n","    # Step 2: Load models\n","    tokenizer_zu, model_zu, device_zu = load_model(\"MoseliMotsoehli/zuBERTa\")\n","    tokenizer_en, model_en, device_en = load_model(\"bert-base-uncased\")\n","\n","    # Step 3: Compute embeddings\n","    X, Y = [], []\n","    print(\"\\nComputing sentence embeddings...\")\n","    for zu, en in zip(zulu_sentences, english_sentences):\n","        try:\n","            z_emb = get_sentence_embedding(zu, tokenizer_zu, model_zu, device_zu)\n","            e_emb = get_sentence_embedding(en, tokenizer_en, model_en, device_en)\n","            X.append(z_emb)\n","            Y.append(e_emb)\n","        except Exception as e:\n","            print(f\"Skipping pair: {zu[:50]} / {en[:50]} | {e}\")\n","\n","    X = np.array(X)\n","    Y = np.array(Y)\n","    print(f\"Computed embeddings for {len(X)} pairs.\")\n","\n","    # Step 4: Compute linear projection\n","    W = compute_linear_projection(X, Y)\n","    np.save(\"projection_W.npy\", W)\n","\n","    # Step 5: Project a Zulu sentence and predict sentiment\n","    test_sentence = zulu_sentences[0]\n","    projected_embedding = project_sentence(test_sentence, W, tokenizer_zu, model_zu, device_zu)\n","    classifier = DummyEnglishSentimentClassifier()\n","    sentiment = classifier.predict(projected_embedding)\n","\n","    print(\"\\n========================================\")\n","    print(f\"Zulu sentence: {test_sentence}\")\n","    print(\"Projected into English embedding space.\")\n","    print(\"Predicted Sentiment:\", sentiment[0])\n","    print(\"========================================\")\n","\n","    # Step 6: Cosine similarity between projected and English embeddings\n","    projected_all = X @ W\n","    sims = [cosine_similarity([pz], [e])[0, 0] for pz, e in zip(projected_all, Y)]\n","    print(\"Mean cosine similarity between projected Zulu and English embeddings:\", np.mean(sims))\n","\n","\n","# ==========================================================\n","# Entry Point\n","# ==========================================================\n","if __name__ == \"__main__\":\n","    run_pipeline()\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# import torch\n","# import numpy as np\n","# import pandas as pd\n","# import kagglehub\n","# from kagglehub import KaggleDatasetAdapter\n","# from transformers import AutoTokenizer, AutoModel\n","\n","\n","# # ==========================================================\n","# # 1. Model Utilities\n","# # ==========================================================\n","# def load_model(model_name: str):\n","#     \"\"\"Load a pre-trained transformer model and tokenizer.\"\"\"\n","#     print(f\"Loading model: {model_name} ...\")\n","#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n","#     model = AutoModel.from_pretrained(model_name)\n","#     model.eval()\n","#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#     model.to(device)\n","#     return tokenizer, model, device\n","\n","\n","# def get_sentence_embedding(sentence: str, tokenizer, model, device):\n","#     \"\"\"Compute mean-pooled sentence embedding.\"\"\"\n","#     inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n","#     with torch.no_grad():\n","#         outputs = model(**inputs)\n","#         embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n","#     return embedding\n","\n","\n","# # ==========================================================\n","# # 2. Projection Utilities\n","# # ==========================================================\n","# def compute_linear_projection(X, Y):\n","#     \"\"\"Compute least-squares linear projection matrix W.\"\"\"\n","#     print(\"Computing linear projection (least squares)...\")\n","#     return np.linalg.pinv(X) @ Y\n","\n","\n","# def project_sentence(sentence, W, tokenizer, model, device):\n","#     \"\"\"Project a Zulu sentence into English embedding space.\"\"\"\n","#     emb = get_sentence_embedding(sentence, tokenizer, model, device)\n","#     return emb @ W\n","\n","\n","# # ==========================================================\n","# # 3. Dataset Utilities\n","# # ==========================================================\n","# def load_kaggle_zulu_dataset():\n","#     \"\"\"Download and load Zulu-English dataset using pandas for safety.\"\"\"\n","#     print(\"Downloading Kaggle dataset...\")\n","#     df = kagglehub.load_dataset(\n","#         KaggleDatasetAdapter.PANDAS,\n","#         \"olaniyanjulius/zulu-to-english-phrases\",\n","#         \"zulu_english_phrases.jsonl\",\n","#         pandas_kwargs={\"lines\": True}\n","#     )\n","#     print(f\"Loaded {len(df)} sentence pairs from Kaggle dataset.\")\n","\n","#     # Print first 5 Zulu sentences\n","#     print(\"\\nFirst 5 Zulu sentences:\")\n","#     for i, s in enumerate(df[\"Zulu\"][:5], 1):\n","#         print(f\"{i}: {s}\")\n","\n","#     # Print first 5 English sentences\n","#     print(\"\\nFirst 5 English sentences:\")\n","#     for i, s in enumerate(df[\"English\"][:5], 1):\n","#         print(f\"{i}: {s}\")\n","\n","#     return df[\"Zulu\"].tolist(), df[\"English\"].tolist()\n","\n","\n","# # ==========================================================\n","# # 4. Dummy Sentiment Classifier\n","# # ==========================================================\n","# class DummyEnglishSentimentClassifier:\n","#     \"\"\"A trivial sentiment classifier based on sum of vector elements.\"\"\"\n","#     def predict(self, X):\n","#         return [\"positive\" if X.sum() > 0 else \"negative\"]\n","\n","\n","# # ==========================================================\n","# # 5. Main Pipeline\n","# # ==========================================================\n","# def run_pipeline():\n","#     # Step 1: Load dataset\n","#     zulu_sentences, english_sentences = load_kaggle_zulu_dataset()\n","\n","#     # Step 2: Load models\n","#     tokenizer_zu, model_zu, device_zu = load_model(\"MoseliMotsoehli/zuBERTa\")\n","#     tokenizer_en, model_en, device_en = load_model(\"bert-base-uncased\")\n","\n","#     # Step 3: Compute embeddings\n","#     X, Y = [], []\n","#     print(\"Computing sentence embeddings...\")\n","#     for zu, en in zip(zulu_sentences, english_sentences):\n","#         try:\n","#             z_emb = get_sentence_embedding(zu, tokenizer_zu, model_zu, device_zu)\n","#             e_emb = get_sentence_embedding(en, tokenizer_en, model_en, device_en)\n","#             X.append(z_emb)\n","#             Y.append(e_emb)\n","#         except Exception as e:\n","#             print(f\"Skipping pair: {zu} / {en} | {e}\")\n","\n","#     X = np.array(X)\n","#     Y = np.array(Y)\n","#     print(f\"Computed embeddings for {len(X)} pairs.\")\n","\n","#     # Step 4: Compute linear projection\n","#     W = compute_linear_projection(X, Y)\n","#     np.save(\"projection_W.npy\", W)\n","\n","#     # Step 5: Test on a Zulu sentence\n","#     test_sentence = zulu_sentences[0]\n","#     projected_embedding = project_sentence(test_sentence, W, tokenizer_zu, model_zu, device_zu)\n","\n","#     # Step 6: Sentiment prediction\n","#     classifier = DummyEnglishSentimentClassifier()\n","#     sentiment = classifier.predict(projected_embedding)\n","\n","#     print(\"\\n========================================\")\n","#     print(f\"Zulu sentence: {test_sentence}\")\n","#     print(\"Projected into English embedding space.\")\n","#     print(\"Predicted Sentiment:\", sentiment[0])\n","#     print(\"========================================\")\n","#     projected = X @ W\n","#     from sklearn.metrics.pairwise import cosine_similarity\n","#     sims = [cosine_similarity([pz], [e])[0,0] for pz, e in zip(projected, Y)]\n","#     print(\"Mean cosine similarity between projected Zulu and English embeddings:\", np.mean(sims))\n","\n","# # ==========================================================\n","# # Entry Point\n","# # ==========================================================\n","# if __name__ == \"__main__\":\n","#     run_pipeline()\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdoxlTkluynz","executionInfo":{"status":"ok","timestamp":1763000790279,"user_tz":300,"elapsed":556930,"user":{"displayName":"Raveena Gupta","userId":"06103734461678506047"}},"outputId":"749d997b-6ed6-4027-de88-413b22d4d7c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train examples: 67349, Test examples: 872\n","Computing embeddings for training data...\n","Computing embeddings for test data...\n","Train shape: (67349, 768), Test shape: (872, 768)\n","Epoch 1 - Loss: 0.6885 - Val Accuracy: 0.5092\n","Epoch 2 - Loss: 0.6635 - Val Accuracy: 0.5161\n","Epoch 3 - Loss: 0.6421 - Val Accuracy: 0.6743\n","Epoch 4 - Loss: 0.6139 - Val Accuracy: 0.8119\n","Epoch 5 - Loss: 0.5848 - Val Accuracy: 0.7924\n","Epoch 6 - Loss: 0.5590 - Val Accuracy: 0.7993\n","Epoch 7 - Loss: 0.5327 - Val Accuracy: 0.8119\n","Epoch 8 - Loss: 0.5059 - Val Accuracy: 0.8222\n","Epoch 9 - Loss: 0.4824 - Val Accuracy: 0.8142\n","Epoch 10 - Loss: 0.4613 - Val Accuracy: 0.8142\n","Epoch 11 - Loss: 0.4411 - Val Accuracy: 0.8211\n","Epoch 12 - Loss: 0.4242 - Val Accuracy: 0.8268\n","Epoch 13 - Loss: 0.4102 - Val Accuracy: 0.8234\n","Epoch 14 - Loss: 0.3972 - Val Accuracy: 0.8211\n","Epoch 15 - Loss: 0.3861 - Val Accuracy: 0.8211\n","Epoch 16 - Loss: 0.3769 - Val Accuracy: 0.8268\n","Epoch 17 - Loss: 0.3684 - Val Accuracy: 0.8303\n","Epoch 18 - Loss: 0.3615 - Val Accuracy: 0.8383\n","Epoch 19 - Loss: 0.3562 - Val Accuracy: 0.8394\n","Epoch 20 - Loss: 0.3515 - Val Accuracy: 0.8383\n","Epoch 21 - Loss: 0.3480 - Val Accuracy: 0.8383\n","Epoch 22 - Loss: 0.3452 - Val Accuracy: 0.8463\n","Epoch 23 - Loss: 0.3425 - Val Accuracy: 0.8486\n","Epoch 24 - Loss: 0.3406 - Val Accuracy: 0.8498\n","Epoch 25 - Loss: 0.3390 - Val Accuracy: 0.8475\n","Epoch 26 - Loss: 0.3375 - Val Accuracy: 0.8383\n","Epoch 27 - Loss: 0.3365 - Val Accuracy: 0.8417\n","Epoch 28 - Loss: 0.3353 - Val Accuracy: 0.8475\n","Epoch 29 - Loss: 0.3342 - Val Accuracy: 0.8475\n","Epoch 30 - Loss: 0.3332 - Val Accuracy: 0.8498\n","✅ Saved trained English sentiment classifier to 'english_sentiment_classifier.pt'\n","Sentence: 'This is horrible!' -> Sentiment: negative\n"]}],"source":["# Install dependencies (if not already)\n","# !pip install transformers datasets torch scikit-learn\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from transformers import AutoTokenizer, AutoModel\n","from datasets import load_dataset\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","# ==========================================================\n","# 1. Load SST-2 dataset\n","# ==========================================================\n","dataset = load_dataset(\"glue\", \"sst2\")\n","train_data = dataset[\"train\"]\n","test_data = dataset[\"validation\"]\n","\n","train_sentences = train_data[\"sentence\"]\n","train_labels = train_data[\"label\"]  # 0=negative, 1=positive\n","test_sentences = test_data[\"sentence\"]\n","test_labels = test_data[\"label\"]\n","\n","print(f\"Train examples: {len(train_sentences)}, Test examples: {len(test_sentences)}\")\n","\n","# ==========================================================\n","# 2. Load English embedding model (BERT)\n","# ==========================================================\n","tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","model_en = AutoModel.from_pretrained(\"bert-base-uncased\")\n","model_en.eval()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_en.to(device)\n","\n","# ==========================================================\n","# 3. Compute embeddings\n","# ==========================================================\n","def get_sentence_embedding(sentence):\n","    inputs = tokenizer_en(sentence, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n","    with torch.no_grad():\n","        outputs = model_en(**inputs)\n","        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n","    return embedding\n","\n","print(\"Computing embeddings for training data...\")\n","X_train = np.array([get_sentence_embedding(s) for s in train_sentences])\n","y_train = np.array(train_labels)\n","\n","print(\"Computing embeddings for test data...\")\n","X_test = np.array([get_sentence_embedding(s) for s in test_sentences])\n","y_test = np.array(test_labels)\n","\n","print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n","\n","# ==========================================================\n","# 4. Define classifier head\n","# ==========================================================\n","class EnglishClassifierHead(nn.Module):\n","    def __init__(self, embedding_dim):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(embedding_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 2)\n","        )\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","# ==========================================================\n","# 5. Train classifier\n","# ==========================================================\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n","\n","classifier = EnglishClassifierHead(X_train.shape[1])\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(classifier.parameters(), lr=1e-3)\n","\n","for epoch in range(30):  # short demo; increase for better performance\n","    classifier.train()\n","    optimizer.zero_grad()\n","    outputs = classifier(X_train_tensor)\n","    loss = criterion(outputs, y_train_tensor)\n","    loss.backward()\n","    optimizer.step()\n","\n","    classifier.eval()\n","    with torch.no_grad():\n","        preds = torch.argmax(classifier(X_test_tensor), dim=1)\n","        acc = (preds == y_test_tensor).float().mean().item()\n","    print(f\"Epoch {epoch+1} - Loss: {loss.item():.4f} - Val Accuracy: {acc:.4f}\")\n","\n","torch.save(classifier.state_dict(), \"english_sentiment_classifier.pt\")\n","print(\"✅ Saved trained English sentiment classifier to 'english_sentiment_classifier.pt'\")\n","\n","# ==========================================================\n","# 6. Test predictions\n","# ==========================================================\n","test_sentences_demo = [\n","    \"This is horrible!\",\n","]\n","\n","for s in test_sentences_demo:\n","    emb = get_sentence_embedding(s)\n","    emb_tensor = torch.tensor(emb, dtype=torch.float32).unsqueeze(0)\n","    classifier.eval()\n","    with torch.no_grad():\n","        logits = classifier(emb_tensor)\n","        pred = torch.argmax(logits, dim=1).item()\n","        print(f\"Sentence: '{s}' -> Sentiment: {'positive' if pred==1 else 'negative'}\")\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16950,"status":"ok","timestamp":1763002371388,"user":{"displayName":"Raveena Gupta","userId":"06103734461678506047"},"user_tz":300},"id":"4lCFtHrp5BKm","outputId":"28e87a34-cf5d-4f80-8daa-d874b196626c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Projection matrix W shape: (768, 768)\n","Processing Zulu phrases and predicting sentiment...\n","\n","\n","✅ Zulu → English → Sentiment Accuracy: 57.20%\n"]}],"source":["import torch\n","import numpy as np\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.metrics import accuracy_score\n","import torch.nn as nn\n","\n","# ===============================================\n","# 1. Load saved projection matrix and English classifier\n","# ===============================================\n","W = np.load(\"projection_W.npy\")  # linear projection matrix\n","print(\"Projection matrix W shape:\", W.shape)\n","\n","class EnglishClassifierHead(nn.Module):\n","    def __init__(self, embedding_dim):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(embedding_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 2)\n","        )\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","embedding_dim = 768\n","classifier = EnglishClassifierHead(embedding_dim)\n","classifier.load_state_dict(torch.load(\"english_sentiment_classifier.pt\", map_location=\"cpu\"))\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","classifier.to(device)\n","classifier.eval()\n","\n","# ===============================================\n","# 2. Load Zulu model\n","# ===============================================\n","tokenizer_zu = AutoTokenizer.from_pretrained(\"MoseliMotsoehli/zuBERTa\")\n","model_zu = AutoModel.from_pretrained(\"MoseliMotsoehli/zuBERTa\")\n","model_zu.to(device)\n","model_zu.eval()\n","\n","# ===============================================\n","# 3. Helper: Sentence embedding\n","# ===============================================\n","def get_sentence_embedding(sentence, tokenizer, model, device):\n","    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        # Check if model outputs last_hidden_state\n","        if hasattr(outputs, \"last_hidden_state\"):\n","            emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n","        else:\n","            raise ValueError(\"Model output missing last_hidden_state\")\n","    return emb\n","\n","# ===============================================\n","# 4. Load Zulu dataset\n","# ===============================================\n","ds = load_dataset(\"michsethowusu/zulu-sentiments-corpus\")\n","zulu_texts = ds[\"train\"][\"Zulu\"]\n","true_labels = ds[\"train\"][\"sentiment\"]\n","label_map = {\"Negative\": 0, \"Positive\": 1}\n","true_labels_numeric = [label_map[l] for l in true_labels]\n","\n","# ===============================================\n","# 5. Predict\n","# ===============================================\n","pred_labels = []\n","\n","print(\"Processing Zulu phrases and predicting sentiment...\\n\")\n","\n","for idx, text in enumerate(zulu_texts[:2000]):  # demo limit\n","    try:\n","        # 1. Get Zulu embedding\n","        z_emb = get_sentence_embedding(text, tokenizer_zu, model_zu, device)\n","        # print(f\"Example {idx} - Zulu emb shape: {z_emb.shape}, mean: {z_emb.mean():.4f}, std: {z_emb.std():.4f}\")\n","\n","        # 2. Project to English embedding space\n","        if z_emb.shape[0] != W.shape[0]:\n","            print(f\"⚠️ Shape mismatch: z_emb {z_emb.shape}, W {W.shape}\")\n","            pred_labels.append(0)\n","            continue\n","\n","        e_emb = np.dot(z_emb, W)\n","        # print(f\"Example {idx} - Projected emb shape: {e_emb.shape}, mean: {e_emb.mean():.4f}, std: {e_emb.std():.4f}\")\n","\n","        # 3. Predict using English classifier\n","        e_emb_tensor = torch.tensor(e_emb, dtype=torch.float32).unsqueeze(0).to(device)\n","        with torch.no_grad():\n","            logits = classifier(e_emb_tensor)\n","            pred = torch.argmax(logits, dim=1).item()\n","        pred_labels.append(pred)\n","\n","        # 4. Print phrase and sentiment\n","        true_sent = true_labels_numeric[idx]\n","        pred_sent_str = \"Positive\" if pred == 1 else \"Negative\"\n","        true_sent_str = \"Positive\" if true_sent == 1 else \"Negative\"\n","        # print(f\"Example {idx}:\\nZulu phrase: {text}\\nPredicted: {pred_sent_str}, True: {true_sent_str}\")\n","        # print(\"──────────────────────────────────────────────\")\n","\n","    except Exception as e:\n","        print(f\"Skipping example {idx} due to error: {e}\")\n","        pred_labels.append(0)\n","\n","# ===============================================\n","# 6. Compute accuracy\n","# ===============================================\n","acc = accuracy_score(true_labels_numeric[:len(pred_labels)], pred_labels)\n","print(f\"\\n✅ Zulu → English → Sentiment Accuracy: {acc * 100:.2f}%\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOehm2gmW+/Fv2DZajhpu3W"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}